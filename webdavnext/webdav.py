"""Instantly find and access all your cloud data"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/00_exploring-your-remote-data.ipynb.

# %% auto 0
__all__ = ['node_to_dataframe', 'RemoteData']

# %% ../notebooks/00_exploring-your-remote-data.ipynb 15
from nc_py_api import Nextcloud 

#from webdav3.client import Client
#import webdav3 

import polars as pl 
import itables
from itables.widget import ITable
import os 
from pathlib import Path 
import time
from dateutil import parser as dateutil_parser
import re 
from IPython.display import HTML, display

# %% ../notebooks/00_exploring-your-remote-data.ipynb 16
def node_to_dataframe(fsnode): 
    '''Convert `fsnode` object to polars a single row polars dataframe.'''

    df = pl.DataFrame({'path': fsnode.user_path, 'size': fsnode.info.size, 'mimetype': fsnode.info.mimetype, 'modified': fsnode.info.last_modified, 
                   'isdir': fsnode.is_dir, 'ext': os.path.splitext(fsnode.user_path)[1]})

    return df 


class RemoteData(object): 
    
    # Setting the `Depth` parameter to `infinity` is important to recursively list the file tree in one go. 
    # See: http://webdav.org/specs/rfc4918.html#METHOD_PROPFIND

    
    #Client.default_http_header['list'] = ['Accept: */*', 'Depth: infinity'] 

    itables.options.maxBytes = 0
    itables.init_notebook_mode()

    def __init__(self, configuration): 
        '''Recursively scan the contents of a remote webdav server as specified by `configuration`. 
        '''

        options = configuration.copy()
        remote_path = options.pop('remote_path')

        print(f'Please wait while scanning all file paths in remote folder...')
        
        # Nextcloud 
        
        self.nc = Nextcloud(**options) 

        # query webdav server to obtain file listing 
        fs_nodes_list = self.nc.files.listdir(remote_path, depth=-1, exclude_self=False) 
        
        #info = self.client.list(remote_path=remote_path, get_info=True)

        n_paths = len(fs_nodes_list)

        # load into polars 

        # initialize dataframe with first row to fix schema 
        self.df = node_to_dataframe(fs_nodes_list[0])

        for fsnode in fs_nodes_list[1:]: 
            self.df.extend(node_to_dataframe(fsnode))
        

        # keep relevant columns, remove path prefixes and add extensions column 
        #self.df = df_full[['path', 'size', 'content_type', 'modified', 'isdir']]
        #path_list = [path for path in list(self.df['path'])] 
        #ext_list = pl.DataFrame({'ext': [os.path.splitext(path)[1] for path in path_list]})
        #self.df = self.df.with_columns(ext_list['ext'].alias('ext'))
        #self.df = self.df.with_columns(pl.col('path').str.replace('/remote.php/dav/files/asap-public-webdav/', ''))

        # create interactive table 
        self.table = ITable(
                    self.df,
                    layout={"top1": "searchBuilder"},
                    select=True,
                    searchBuilder={}, 
                    scrollY="500px", scrollCollapse=True, paging=False, 
                ) 

        print(f"Ready building file table for '{remote_path}', Total number of files and directories: {n_paths}   ")

    
    def download_selected(self, cache_dir=None): 
        '''Download selected files (blue rows) from `table` to local cache directory `cache_dir`.'''
        
        # create cache directory 
        if cache_dir is None: 
            cache_dir = Path.home().joinpath('.cache')
    
        os.makedirs(cache_dir, exist_ok=True)
    
        # obtain remote paths and remote timestamps 
        remote_path_list = [self.table.df['path'][n] for n in self.table.selected_rows]
        remote_modified_list = [self.table.df['modified'][n] for n in self.table.selected_rows]
        remote_isdir_list = [self.table.df['isdir'][n] for n in self.table.selected_rows]
        
        n_files = len(remote_path_list)
       
        for i, [remote_path, remote_modified, remote_isdir] in enumerate(zip(remote_path_list, remote_modified_list, remote_isdir_list)): 
    
            # only download actual files 
            if not remote_isdir:   
                remote_directory = os.path.dirname(remote_path)
                local_directory = cache_dir.joinpath(remote_directory) # I guess this will not yet work for Windows
                
                # create directory structure inside cache 
                os.makedirs(local_directory, exist_ok=True) 
            
                # get remote epoch time 
                remote_modified_epoch_time = int(dateutil_parser.parse(remote_modified).timestamp()) 
            
                # infer local path 
                local_path = cache_dir.joinpath(remote_path) 
            
                # check if local file exists and if modification times are similar 
                is_local = local_path.exists()  
            
                is_similar = False 
                if is_local: 
                    local_modified_epoch_time = int(os.stat(local_path).st_mtime)
            
                    if local_modified_epoch_time == remote_modified_epoch_time: 
                        is_similar = True 
            
                if not is_similar: 
                    print(f'[{i}/{n_files - 1}] Downloading to: {local_path}                         ', end='\r')
                    self.client.download(remote_path, local_path) 
                
                    now = int(time.time())
                    os.utime(local_path, (now, remote_modified_epoch_time)) 
                    
        print(f"Ready with downloading selected remote data to local cache: {cache_dir}/{self.table.df['path'][0]}                                                                       ")


